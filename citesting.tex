% !TEX program = xelatex
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\kacper}[1]{ \bf  { \color{Orchid}{Kc: #1}}  }

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{statement}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem{test}{Test}
\newtheorem{proposition}{Proposition}

\newtheorem*{remark}{Remark}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

\title{}
\author{KC}
\date{}



\begin{document}

\maketitle

\section{}


\paragraph{Fixing the proof of testability.}
\cite{bergsma2004testing} introduces a concept of testability in attempt to prove that conditional independence is not testable, however there is a mistake in the proof.



\paragraph{Alternative proof.}
All the equations (for functions) in the following note  hold almost everywhere, unless explicitly specified otherwise.
\begin{definition}
 A parameter  $\theta$ of a probability measure $p \in \mathcal P$, where $\mathcal P$ is a set of probability measures, is a function $\mathcal P  \to  \mathbf R$.   
\end{definition}


% According to \cite{bergsma2004testing} this definition is by Hoeffding (to be verified). 
\begin{definition}[Hoeffding]
A parameter  $\theta$ is called estimable of degree $r$, if $r$ is the smallest number for which there is a bounded function $h : \mathbf R^{r} \to \mathbf R$ such that, for all $p \in \mathcal P$, $ \theta(p) = Eh(X_1, \cdots , X_r)$ (exactly),  if $X_i$ are iid random variables sampled according to $p$.
\end{definition}

\cite{bergsma2004testing} introduces a concept of testability which is a 'syntactic sugar' on the concept of estimable parameter.

\begin{definition}[testability, \cite{bergsma2004testing}]
 Let $\mathcal{P}$ be a set of probability measures and $\mathcal{P}'$ its complement. Let $\Theta$ be a family of  estimable parameters which are zero on all elements of $\mathcal{P}$.  $\mathcal{P}$ is non-testable if and only if $\Theta$ contains only $0$ function. Otherwise $\mathcal{P}$ is testable. 
\end{definition}



Non-testability implies that a statistical test based on $U$-statistics can not be conducted, which does not rule out, as remarked by G. Peters, possibility of creating tested based on some other estimators e.g. M-estimator. The lemmas we give next are a used in the proof of the Theorem  \ref{th:1}, stating non-testability of conditional independence. 

\begin{lemma}
\label{lem:aha}
 The only polynomial of $m$ variables that is zero on the $m$-dimensional simplex is $\sum_{i=1}^m x_i-1$ and $0$.  
\end{lemma}
\begin{proof}
Consider restriction of the polynomial on the $R^m$ space  the affine space $\sum_{i=1}^m x_i=1$  -- this restriction is a polynomial of $m-1$ variables as seen by substitution $x_1 = 1 -\sum_{i=2}^m x_i$. This restriction zero is on the simplex $0 \leq x_i \leq 1$, therefore it must be zero on the whole affine space  $\sum_{i=1}^m x_i=1$. This is due to the fact that set of zeros of the polynomial is nowhere dense on its domain. The only polynomials that are zero on a set $\sum_{i=1}^m x_i=1$ are $\sum_{i=1}^m x_i-1$ and $0$.   
\end{proof}


\begin{lemma}
\label{the:Lemma}
Let $h: R^r \to R$ be a bounded function, symmetric in its arguments, i.e. for any permutation $\pi$ of set $(1,\cdots,r)$,
$$
h(x_{\pi(1)},\cdots,x_{\pi(1)}) = h(x_1,\cdots,x_r).   
$$
If for any sequence of iid random variables $(Z_1,\cdots,Z_r)$ the expected value of $h$ is zero, i.e.
$$
E h(Z_1,\cdots,Z_r) = 0,
$$
then $h$ is equal to zero.
\end{lemma}



\begin{proof}
Assume to the contrary that there exists a function $h$ and the set of a positive measure $A$, such that $\int_{A} h^2 > \epsilon^2$ and   
$$
E h(Z_1,\cdots,Z_r) = 0,
$$
for all id random variables $(Z_1,\cdots,Z_r)$.

\textbf{Outline.} We first construct a function $g$ that  approximates function $h$ on some hypercube $C$ contained in  the set $A$.  $\epsilon$-close approximation ($\| h-g\|_{L_2(C)} \leq \epsilon^2$) will be piecewise constant on small  $r$-dimensional hypercubes; additionally, for a specific family of random variables $\mathcal{F}$, the property $E g(Z_1,\cdots,Z_r)=0, Z \in \mathcal{F}$ will hold. Next we will show that $g=0$ and conclude that $\| h\|_{L_2(C)}\leq \epsilon$. The contradiction comes from the fact that $\| h\|_{L_2(C)}\leq \epsilon$ but by the assumption $\int_{A} h^2 > \epsilon^2$.  

Let $C=[-a,a]^r$ be a hypercube that contains the set $A$. For an interval $[-a,a]$ and some integer $m>0$, let $x_i = a(-1 + \frac{2i}{m})$, for $0 \leq i \leq m$, be a sequence of points that divide the interval $[-a,a]$ into subintervals of an equal length $\frac{2}{m}$. Since $h$ is bounded, there exists $M$ such that for all $m>M$, $h$ can be approximated using step functions on the sub-hypercubes of the hypercube $C$ with the grid defined by the sequence $x_i$. The vertices of the sub-hypercubes are $r$ dimensional points in the set  $\{ (x_{i_1},x_{i_1+1}) \times \cdot \times (x_{i_r},x_{i_r+1})\}$ for a sequence of the indexes $0 \leq i_1, \cdots, i_r\leq m$. By construction, the approximation of function  $h$,  function $g$, is $\epsilon$ close to $h$ in squared norm, that is
 \[
  \int_{C} \left( h(z_i,\cdot,z_r) -g(z_i,\cdot,z_r) \right)^2 d z_1 \cdots d z_r \leq \epsilon^2.
 \]
 
 
Let us define a family of functions $\{e_i\}_{0\leq i \leq m}$, constant on the intervals $[x_i,x_{i+1}]$
\[
e_i(z) = 1_{z \in [x_i,x_{i+1}]} \sqrt{\frac{m}{2}}.
\]
Let $\mathcal H$ be a space of functions spanned by linear combinations of $e_i$, that is  $f = \sum_{i=1}^{m} \alpha_i e_i$ for some sequence $\alpha_m$. This space, being isometrically isomorphic to $m$-dimensional Euclidean space,  is endowed with an the inner product 
\[
\langle e_i,e_j \rangle_{\mathcal H} = \delta_{i,j}.
\]
This inner product coincides with the inner product on the space of square integrable functions on the interval $[-a,a]$, 
\begin{align}
\langle e_i,e_i \rangle_{L_2} &= \int_{-a}^{a} e_i(x) e_i(x) = \frac{2}{m} \sqrt{\frac m 2}^2 =1,  \\ 
\langle e_i,e_j \rangle_{L_2} &= \int_{-a}^{a} e_i(x) e_j(x) = 0. 
\end{align}
Consequently, for any two functions $f_1,f_2$ in $\mathcal{H}$, we have 
\[
\langle f_1,f_2 \rangle_{L_2} =  \langle f_1,f_2 \rangle_{\mathcal{H}}.
\]
Formally speaking, $f \in \mathcal{H}$ is not the same as $f \in \mathcal L_2$, but we will not  introduce extra notation and additionally  we will drop subscripts in the inner products. 

Let $\mathcal H^r$ be a space of function spanned by basis $e_{i_1}(z_1) \cdots  e_{i_r}(z_r)$ for for $0\leq i_1,\cdots,i_r\leq m$. Similarly to $\mathcal{H}$, $\mathcal H^r$ is  isometrically isomorphic to  the $r \times m$ dimensional Euclidean space, and its inner product coincides with an inner product on the $L_2(C)$ space. 

Using basis $e_{i_1}(z_1) \cdots  e_{i_r}(z_r)$,  we construct the best (with respect to squared norm) piecewise  constant  approximation of $h$.  The best constant approximation of a function, over some set, is its mean. For any small hypercube $S =  \{ (x_{i_1},x_{i_1+1}) \times \cdot \times (x_{i_r},x_{i_r+1})\}$ we set $g$ to a mean value of $h$ over this hypercube; formally we set coefficients $\beta_{i_1,\cdots,i_r}$ to normalized mean value and use them to define $g$
\[
\beta_{i_1,\cdots,i_r} = \frac{ \int_{S} h(z_{i_1},\cdots,z_{i_r} ) dz_1 \cdots dz_r} { (2/m)^r} 
\]
\[
 g(z_1,\cdots z_r)= \sum_{i_i=1}^m \cdots \sum_{i_r=1}^m \beta_{i_1,\cdots,i_r} e_{i_1}(z_1) \cdots  e_{i_r}(z_r).
\]
By the mean property, for any $C$
\begin{align}
\int_{S} C h(z_{i_1},\cdots,z_{i_r} ) dz_1 \cdots dz_r= \int_{S} C g(z_{i_1},\cdots,z_{i_r} ) dz_1 \cdots dz_r.
\end{align}
Therefore any piecewise constant function $f \in \mathcal{H}^r$ 
\begin{align}
\int_{C}& h(z_{i_1},\cdots,z_{i_r}) f(z_{i_1},\cdots,z_{i_r}) dz_1 \cdots dz_r = \\
\int_{C}& g(z_{i_1},\cdots,z_{i_r}) f(z_{i_1},\cdots,z_{i_r})dz_1 \cdots dz_r=\\
&0, 
\end{align}
which, as outlined above, implies $\langle h,f \rangle = \langle g,f \rangle =0$. 


Since $h$ is symmetric, $g$ is also symmetric, and therefore values of coefficients $\beta_{i_1,\cdots,i_r}$ are constant under permutations,  
\[
 \beta_{i_1,\cdots,i_r} = \beta_{\pi(i_1,\cdots,i_r)},
\]
where $\pi$ is permutation of $r$ elements. 

Consider the family of density functions  $\mathcal F \subset \mathcal H^r$, such that  
\[
f(z_1,\cdots,z_r)= (\sum_{i=1}^{m} \alpha_i e_i(z_1)) \cdots (\sum_{i=1}^{m} \alpha_i e_i(z_r)),
\]
$\alpha_i \geq 0$ and $\sum_{i=1}^m \alpha_i=1$. Since $\mathcal F \subset \mathcal H^r$, for all random variable $Z$ with density  $f \in \mathcal{F}$ we have $E g(Z_1,\cdots ,Z_r) = \langle g, f \rangle =0$. The inner product $\langle g, f \rangle$ allows explicit representation  
\begin{align}
\label{eq:fatality}
\langle g, f \rangle = \sum_{i_1=1}^{m} \cdots  \sum_{i_r=1}^{m} \beta_{i_1,\cdots, i_r} \alpha_{i_1} \cdots \alpha_{i_r}.
\end{align}
The above equation \eqref{eq:fatality} is a polynomial of $m$ variables $\alpha_{1},\cdots,\alpha_{m}$. Since $\beta_{i_1,\cdots, i_r}$ are symmetric under permutations, the coefficients of this polynomial are non-zero if and only if $\beta_{i_1,\cdots, i_r}$ are non-zero. By assumption on $\alpha$'s ( $\alpha_i \geq 0$, and $\sum_{i=1}^m \alpha_i=1$) and the assumption $\langle g, f \rangle =0$, this polynomial is zero on the m-dimensional simplex. By Lemma \ref{lem:aha} the only such polynomials is $\beta_{i_1, \cdots ,i_r}'=0$ -- it can not be a polynomial $\sum \alpha_i=1$, since the polynomial \eqref{eq:fatality} does not have a constant term. 
\end{proof}





\begin{Theorem}
\label{th:1}
 Suppose that the random vector $X,Y,Z$ has a density $p$ and $Z$ has no atoms. The property of conditional independence $p(x,y|z) = p(x|z)p(y|z)$ is not testable of any order. 
\end{Theorem}

\begin{proof}
Assume to the contrary that there exist $h$ such that 
$$
E h(X_1,Y_1,Z_1, \cdots, X_r,Y_r,Z_r) =0 $$ 
if and only if conditional independence holds. By the assumption  
\begin{align}
 E& h(X_1,Y_1,Z_1, \cdots, X_r,Y_r,Z_r) = \\
 E& \left( E h(X_1,Y_1,Z_1, \cdots, X_r,Y_r,Z_r | Z_1,\cdots,Z_r) \right) = \\
 \int& \left(\int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r p(x_i|z_i)p(y_i|z_i) dx_i dy_i  \right)  \prod_{i=1}^r p(z_i)dz_i
\end{align}

Denote $g_{\mu}( Z_1,\cdots,Z_r) = E h(X_1,Y_1,Z_1, \cdots, X_r,Y_r,Z_r | Z_1,\cdots,Z_r)$. We write $\mu$ to remember that $g$ depends on $p(x|z)p(y|z)$. We show that $g_{\mu}$ is symmetric with respect to its arguments. 
\begin{align}
&g_{\mu}(Z_{\pi(1)}, \cdots,Z_{\pi(r)} )  = \\ 
&E h(X_{\pi(1)},Y_{\pi(1)},Z_{\pi(1)}, \cdots, X_{\pi(r)},Y_{\pi(r)},Z_{\pi(r)} | Z_{\pi(1)},\cdots,Z_{\pi(r)}) = \\
&E h(X_1,Y_1,Z_1, \cdots, X_r,Y_r,Z_r | Z_1,\cdots,Z_r) = \\
&g_{\mu}(Z_1, \cdots,Z_r ). 
\end{align}

For the fixed functions $p(x|z)p(y|z)$, the marginal distribution of $Z$ can be chosen  arbitrarily.   By Lemma \ref{the:Lemma}, $g_{\mu}=0$ for all $p(x|z)p(y|z)$.  

The inner conditional expected value is 0 for almost all fixed values of $z_1, \cdots,z_r$. We only need to consider  the case   $z_i\neq z_j$ for $i\neq j$, since $P(z_i= z_j)=0$. 
\begin{align}
\label{ref:eqg}
 \int &h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r)  \prod_{i=1}^{r} f(x_i|z_i)f(y_i|z_i) dx_i dy_i = 0.
\end{align}
Let $I$ be a set of all intervals on the real line. Let  $T$ be all family of all functions $R \to I^2$. Consider a family of densities $\mathcal G$
\[
 \mathcal G = \bigg \{ 1\{ (x,y) \in t(z) \}  \frac{1}{vol(t(z))} p(z) | t \in T \bigg \}
\]
where $p(z)$ is some density. All $g \in \mathcal G$ are conditionally independent. For any element $g \in \mathcal G$.
\begin{align}
&\int h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r} 1\{ (x,y) \in t(z) \}  \frac{1}{vol(t(z))} p(z)dx_i dy_i =\\ 
&\int_{a_1}^{b_1} \int_{c_1}^{d_2} \cdots \int_{a_r}^{b_r} \int_{c_r}^{d_r} h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r}  \frac{dx_i dy_i}{(b_i - a_i)(d_i-c_i) }. \\   
\end{align}
where $(a_i,b_i) \times (c_i,d_i) = t(z_i)$. 
By equation \ref{ref:eqg} we have 
\begin{align}
 \int_{a_1}^{b_1} \int_{c_1}^{d_2} \cdots \int_{a_r}^{b_r} \int_{c_r}^{d_r}& h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r}  dx_i dy_i = 0.
\end{align}
Since for any  sequence $ (a_i,b_i)\times (c_i,d_i) $, there exists  $t: t(z_i) = (a_i,b_i)\times (c_i,d_i) $, $h=0$. 
\end{proof}


\paragraph{Mistake in the proof in \cite{bergsma2004testing}.}
In the proof of the Theorem 2 it is assumed that marginal distribution of conditioning variable has no atoms (in this case $Z$ and $Y$ are  conditionally independent given $X$). Then he claims that for any choice of points ($r$ is fixed)
\[ (x_1 , y_1 , z_1 ), \cdots, (x_r , y_r , z_r ) \]  
such that $x$'s are unique, there exists a measure $P$ such that
\[
P[(x_1 , y_1 , z_1 ), \cdots , (x_r , y_r , z_r ) ] =1. 
\]
I am not sure what this notation  means, I assume it denotes 
\[
P[(X_1=x_1 ,Y_1= y_1 , Z_1=z_1 ), \cdots , (X_r = x_r , Y_r= y_r ,Z_r z_r ) ] ) =1
\]
It is claimed that such a family is a subset of family of conditionally independent independent triples $Z,Y,Z$.
\[
P( (x_1 , y_1 , z_1 ), \cdots , (x_r , y_r , z_r ) ) \leq  P(x_1,\cdots,x_n) =0
\]
The last equality is by the assumption that there are no atoms on $X$ axis.

% 
% \paragraph{conditional covariance}
% The same argument as above works for the conditional covariance. The family $\mathcal G$ used in the proof of the Theorem \ref{th:1} is a family of densities for which $E(XY|Z)=E(X|Z)E(Y|Z)$. 
% 
% \paragraph{Literature overview}
% 
% \paragraph{regression like approach}
% This one \cite{song2007testing} requires strong assumptions on conditional  expected values convergence in the Theorem one, I think. \cite{su2008nonparametric} clear form their th 3.1 (use kde as well). same \cite{fukumizu2007kernel,zhang2012kernel}.
% \cite{huang2010testing}.
% 
% 
% \paragraph{mixing events with sigma fields}
% \cite{gyorfi2012strongly}
% 
% 
% \cite{linton1996conditional} 'In particular, a smoothing based test would not be able to detect
% alternatives at distance $n^{0.5}$ from the null detected by parametric and
% some nonparametric test statistics.
% Our strategy to avoid this dimensionality problem is to verify the
% relationship (5) over subsets (with positive Lebesgue measure) in the
% support of Y , X , Z rather than at individual values, thereby replacing
% densities with distribution functions. Expression (5) is extended to
% $P (C )P (A,B, C ) = P (A,C )P (B,C )$,
% for all subsets A, B in the support of Y , X , and C a subset in the open
% support of Z.
% 
% 
% 
% \paragraph{Other approach that assumes continuity also partially works}
% 
% Following \cite{bergsma2014consistent} we define a discrepancy function
% \begin{equation}
% a(p_1,p_2,p_4,p_4) = |p_1-p_2| -|p_1-p_3|-|p_2-p_4|+|p_4-p_3|.
% \end{equation}
% and 
% \begin{equation}
% \tau = Ea(X_1,X_2,X_3,X_4)a(Y_1,Y_2,Y_3,Y_4).
% \end{equation}
% which is useful in characterizing independence, as seen by 
% \begin{Theorem}[\cite{bergsma2014consistent} ]
% \label{th:tau_star}
% Let $P$ be a distribution on $R^2$, with a density.  $\tau = 0$ if and only if $X_i$ and $Y_i$ independent for  $(X_i,Y_i) \sim P$. 
% \end{Theorem}
% 
% We adopt $\tau$ for the conditional independence testing. Given samples $(X_i,Y_i,Z_i)$ we define  $X^{i},Y^{i},Z^{i}$ to be a sequence of observations ordered increasingly by $Z_i$ -- we will call these the order statistics with respect to $Z$ . Let
%  \[ 
% \tau_i = sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})).  
%  \]
% We assume that if $Z$ is close to $Z'$ then  distribution $X,Y|Z$ is close to  $X',Y'|Z'$ in some sense; specifically  there exist a number $p$,  random variables  $\bar X'$, $\bar Y'$ such that $P( |X'- \bar X'|>\epsilon,|Y'- \bar Y'|>\epsilon  ) \leq P(|Z-Z'|^p>\epsilon)$ and $\bar X', \bar Y'$ are distributed as $X,Y$. For now we will call such distributions 'p-good'. We will show that, under this assumption, $\tau_i$'s will asymptotically behave like $\tau$.   
% 
% We adopt this particular test since it is distribution free, and the test static has easy distribution under the null hypothesis. For example, should we use measure like person correlation, we would have to estimate the conditional variance to calculate the correlation. In contrast, the distribution of $\tau$ has aways constant variance, equal to one, and under the null hypothesis probability of one and minus one are the same.  We start with an auxiliary facts and lemma
% \begin{statement}
% \label{lem:err}
% For any numbers $(z_i,z_i')$, 
% \[
% | a(z_1,z_2,z_4,z_4) -  a(z_1',z_2',z_4',z_4')| \leq 2\sum_{i=1}^4 |z_i-z_i'|
% \]
% \end{statement}
% 
% 
% \begin{lemma}
% \label{lem:bnd}
% Let $(A_n,B_n,E_n)$ be a sequence of random variables such that  $|A_n-B_n| \leq |E_n|$. For any sequence $\delta_n$
% \[
% \left | E  sgn(A_n) - sgn(B_n) \right | \leq  2P(|E_n|>\delta_n) + 2 P(|E_n| < \delta_n ,|A_n| < \delta_n)
% \]
% \end{lemma} 
% \begin{proof}
% We will bound the difference $E sgn(A_n) - sgn(B_n)$,
% \begin{align}
% E&  sgn(A_n) - sgn(B_n)= \\
% &E \mathbf  1 \{ |E_n| < \delta_n ) , |B_n| > \delta_n \} 0 +\\
% &E \mathbf 1 \{ |E_n| < \delta_n)  , |B_n| < \delta_n\}  (sgn(A_n) - sgn(B_n)) + \\
% &E \mathbf 1 \{ |E_n| > \delta_n  \}  (sgn(A_n) - sgn(B_n)).  
%  \end{align}
% The last two terms can be bounded
%  \begin{align}
% &   |E\mathbf 1 \{ |E_n| > \delta_n   \}  sgn(A_n) - sgn(B_n)| \leq 2 P( |E_n|>\delta_n) , \\
% &   |E \mathbf 1 \{ |E_n| < \delta_n   , |B_n| < \delta  \}  sgn(A_n) - sgn(B_n) | \leq 2 P(|E_n| < \delta_n ,|A_n| < \delta_n) . 
% \end{align}
% \end{proof}
% 
% 
% 
% 
% \begin{statement}
% \label{lemma:keyLemma}
%  for any real numbers $a,b,c,d$, 
%  \[
%   2(ab -cd) = (a-c)(b+d) + (a+c)(b-d). 
%  \]
% \end{statement}
% 
% % \begin{definition}
% % Let  $X_i,Y_i,Z_i$ be a sequence such that $P( |X_i-X_j|>\epsilon,|Y_i-Y_j|>\epsilon  ) \leq P(|Z_i-Z_j|^p>\epsilon)$. 
% % Let $W^{i},Z^{i}$ be order statistics by $Z$. We call them p-good if for any $i$ there exist a sequence of independent random variables $\bar W^i$ such that 
% % for any $i$, and $j \in {0,1,2,3}$
% % \[
% %  P( |\bar W^{i+j} - W^{i+j}| > a) \leq P( |Z^{i} - Z^{i+j}|^p >a),
% % \]
% % ,$\bar W^{i+j}$ is distributed as $W^{i}$. 
% % \end{definition}
% 
% \begin{Theorem}
% If $X_i,Y_i,Z_i$ is are 'p-good' and $P( |X| <t) <Ct$, $P( |Y| <t) <Ct$ for small $t$ and $Z$ is uniform over $[0,1]$, then
% \[
%  \frac{1}{\sqrt n} \sum_{j=0;j=+4;n} \tau_j   
% \]
% converges to standard normal distribution  if and only iff  $X_i,Y_i,Z_i$ are conditionally independent.
% \end{Theorem}
% \begin{proof}
% Since $Var(\tau_j)=1$, the sum 
% \[
%  \frac{1}{\sqrt {n/4}} \sum_{i=0;i=+4;n} (\tau_i - E \tau_i)   
% \]
% converges to standard  normal distribution. It is sufficient to prove that $\frac{1} {\sqrt n} \sum_{i}^{n} E \tau_i=0$ converges to zero if and only if  null hypothesis holds. For any  $i=4k$, where $k$ is natural, and $j \in {0,1,2,3}$ let $\bar X_{i+j}$ and $\bar Y_{i+j}$ be independent random variables provided by the 'p-good' assumption i.e. 
% \[
%  P( |X^{i+j}- \bar X^{i+j}|>\epsilon,|Y^{i+j}- \bar Y^{i+j}|>\epsilon  ) \leq P(|Z^i-Z^{i+j}|^p>\epsilon),
% \]
% and $\bar X^{i+j},\bar Y^{i+j}$ are distributed as $X^i,Y^i$. 
% %%TODO give a better definition above to make sure all is independent.
% 
% Define 
% \[
%  \bar \tau_j = sgn(a(\bar X^{j},\bar X^{j+1},\bar X^{j+2}, \bar X^{j+3})) sgn(a(\bar Y^{j},\bar Y^{j+1},\bar Y^{j+2},\bar Y^{j+3}))
% \]
% By \ref{th:tau_star} $E \bar \tau_j=0$ if and only if  $\bar X_i , \bar Y_i$ are independent. Therefore, under the null hypothesis it is sufficient to show  that $ E (\bar \tau_j - \tau_j) = O(1/ \sqrt n)$. Indeed, 
% \[
%  \frac{1} {\sqrt n} \sum_{i}^{n} E (\tau_i - \bar \tau_j +\bar \tau_j) = \frac{1} {\sqrt n} \sum_{i}^{n} O(1/ \sqrt n) +0 \to 0.
% \]
% By the observation \ref{lemma:keyLemma} we see it is sufficient to prove that  
% \begin{align}
%    E sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) - sgn(a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})) \to a_i  \\
%   \sqrt n  E sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})) - sgn(a(\bar Y^{i},\bar Y^{i+1},\bar Y^{i+2}, \bar Y^{i+3})) \to a_i
% \end{align}
% Without loss of generality we can consider only $X$'s. Let $A_n = a(X^{i},X^{i+1},X^{i+2},X^{i+3})$, $B_n = a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})$ . By the lemma \ref{lem:err} $ A_n-B_n \leq E_n = 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}|$. By the lemma  \ref{lem:bnd} it is enough to show that  $P( E_n > \delta_n ) = O(1/ \sqrt n)$ and $P( |B_n| < \delta_n )=O(1/ \sqrt n)$ for some  $\delta_n$. We choose $\delta_n = \frac{C} {n}$. For $B_n$ we have 
% \[
%  P( |B_n| < \frac{1}{\sqrt n} ) \leq \frac{C}{\sqrt n}. 
% \]
% By p-good assumption
% \begin{align}
%  P(|E_i| > \frac{1}{\sqrt n} ) = P( 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}| > \frac{1}{\sqrt n} ) \leq \\
%  P( 8|Z^i - Z^{i+4}|^p > \frac{1}{\sqrt n} )   =  P(  8|Z^i - Z^{i+4}| > n^{-1/2p} ).
% \end{align}
% By (reference)  $D_{n,i} = |Z^i - Z^{i+4}|$ is distributed as $Beta(4,n-3)$.  One can show that for $p>0.5$ prove
% \[
%  \sqrt n P( n D_{n,i} > n^{-1/2p+1}) \to 0 
% \]
% Simple way to see it is true is to  notice that $n D(n,i)$ coverages to gamma(k,1) and we need $-1/2p+1 >0 $ so $n^{-1/2p+1}$ grows (any polynomial growth is OK since gamma has heavy tail).  $-1/2p+1 >0$ is equivalent to $p>0.5$. This is not proof since we used limit twice.
% \end{proof}
% 
% How to construct $\bar X^{i+j}$ ? We set $\bar X^{i+j} =  F^i(F^{i+j}(X^{i+j})$. Let 
% \[
%  P( |\bar X^{i+j} -X^{i+j}  |>a) = P( |F^i(F^{i+j}(X^{i+j}) -X^{i+j}  |>a) 
% \]
% clearly we need to make some assumptions  of $F$, for example of the following should do:  if $|z-z'| \leq a $
% \[
%  |x-x'| 
% \]







\bibliographystyle{plain}
\bibliography{acc}


\end{document}


