% !TEX program = xelatex
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\kacper}[1]{ \bf  { \color{Orchid}{Kc: #1}}  }

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{statement}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem{test}{Test}
\newtheorem{proposition}{Proposition}

\newtheorem*{remark}{Remark}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

\title{}
\author{KC}
\date{}



\begin{document}

\maketitle





\paragraph{Alternative proof.}


\begin{definition}
 A parameter  $\theta$ of a probability measure $p \in \mathcal P$, where $\mathcal P$ is a set of probability measures, is a function $\mathcal P  \to  \mathbf R$.   
\end{definition}


% According to \cite{bergsma2004testing} this definition is by Hoeffding (to be verified). 
\begin{definition}[Hoeffding]
A parameter  $\theta$ is called estimable of degree $r$, if $r$ is the smallest number for which there is a bounded function $h : \mathbf R^{r} \to \mathbf R$ such that, for all $p \in \mathcal P$, $ \theta(p) = Eh(X_1, \cdots , X_r)$ (exactly),  if $X_i$ are iid random variables sampled according to $p$.
\end{definition}

\cite{bergsma2004testing} introduces a concept of testability which is a 'syntactic sugar' on the concept of estimable parameter.

\begin{definition}[testability, \cite{bergsma2004testing}]
 Let $\mathcal{P}$ be a set of probability measures and $\mathcal{P}'$ its complement. Let $\Theta$ be a family of  estimable parameters which are zero on all elements of $\mathcal{P}$.  $\mathcal{P}$ is non-testable if and only if $\Theta$ contains only $0$ function. Otherwise $\mathcal{P}$ is testable. 
\end{definition}



Non-testability implies that a statistical test based on $U$-statistics can not be conducted, which does not rule out, as remarked by G. Peters, possibility of creating tested based on some other estimators e.g. M-estimator. The lemmas we give next are a used in the proof of the Theorem  \ref{th:1}, stating non-testability of conditional independence. 



Let $f(x,y,z)$ be a real valued integrable function. If we skip some of the arguments of $f$ and write e.g. $f(x,z)$ we mean function created by integrating the mising variables out, in this particular example $\int_{R} f(x,y,z) dy$.  


Let $I$ be a set of all intervals on the real line. Let  $T$ be all family of all continuous functions from real line to rectangles i.e. $R \to I^2$ (the norm is inherited form $R^4$, since each rectangle is decoded by four real numbers). Consider a family functions
\begin{align}
 \mathcal G = \bigg \{ 1\{ (x,y) \in t(z) \}p(z) \frac{1}{vol(t(z))} | t \in T_2 , p \in P  \bigg \} 
\end{align}
where $P$ is set of all densities on $R$. If $g \in G$, then 
\[
g(x,y,z) g(z) = g(x,z) g(y,z). 
\]

\begin{lemma}
\label{the:Lemma}
Let $h: R^n \to R$ be a integrable function, symmetric in its arguments, i.e. for any permutation $\pi$ of set $(1,\cdots,n)$,
$$
h(x_{\pi(1)},\cdots,x_{\pi(n)}) = h(x_1,\cdots,x_n).   
$$
If for all probability distributions $p$ on the real line, the integral 
\begin{align}
\label{lb:the_equation}
\int h(z_1,\cdots,z_n) \prod_{i=1}^n p(z_i) d z_i = 0, 
\end{align}
then $h$ is equal to zero everywhere.
\end{lemma}

\begin{proof}
 Consider a probability distribution $p(z) = \sum_{i=1}^{n} \alpha_{i} p_i(z)$, where $\sum_{i=1}^n \alpha_i=1$ , $\alpha_i>0$ and $p_i$ are some probability distributions. Substitute $p$  into the  equation \ref{lb:the_equation}, integrate and denote resulting polynomial by $w$ 
 \[
  w(\alpha) = \sum_{1 \leq i_1, \cdots , i_n \leq n} \alpha_{i_1} \cdots \alpha_{i_n} \beta_{i_1,...,i_n} 
 \]
where $\alpha$ is a vector of $(\alpha_1,\cdots, \alpha_n)$ and $\beta$ coefficients are given by integrals
 \[
  \beta_{i_1,...,i_n} = \int  h(z_1,\cdots,z_n) \prod_{i=1}^n p_{i}(z_i) d z_i.
 \]
 
$w$ is a homogoneus, polynomial of $n$ variables -- in particular,  if $w(\alpha)=0$ then for all $\lambda$, $w(\alpha x)= \lambda w(\alpha)=0$.  Therefore, since $w$ is equal zero on the standard simplex, it is equal to zero on  positive orthant. This implies that $w$ is equal to zero everywhere. 

Since $h$  is symmetric, the coefficient of the term $\alpha_{1},\cdots, \alpha_{n}$ is equal to $n! \beta_{1,\cdots, n}$, which implies, by $w$ being equal to zero, that 
\[
  \beta_{1,\cdots, n} = \int  h(z_1,\cdots,z_n) \prod_{i=1}^n p_{i}(z_i) d z_i =0.
\]
$p_{i}$'s are arbitrary distributions and so  $h$ is zero everywhere.
\end{proof}


\begin{Theorem}
\label{th:1}
If a integrable, symmetric function $h : R^{3n} \to R$ is such that 
\[
 \int_{R^{3n}} h(x_1,y_1,z_1,\cdots,x_n,y_n,z_n) \prod_{i=1}^n p(x_i,y_i,z_i) d x_i d y_i d z_i = 0
\]
for all $p \in \mathcal G$, then $h=0$ almost everywhere.
\end{Theorem}

\begin{proof}
Assume to the contrary that such an $h \neq 0 $ exists. By the Fubini theorem  
\begin{align}
\int  \left( \int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r \frac{ 1_{ (x_i,y_i) } \in t(z_i) }{ vol(t(z_i)) }   d x_i d y_i \right) \prod_{i=1}^r  p(z_i)  d z_i
\end{align}
Inner integral depends only on $t$ and does not depend on $p$. Denote 
\[
g_{t}( z_1,\cdots,z_r)  = \int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r \frac{ 1_{ (x_i,y_i) } \in t(z_i) }{ vol(t(z_i)) }    d x_i d y_i
\]
 By Lemma \ref{the:Lemma} applied to $g_t$ ($g_{t}$ is symmetric and integrable), for any $z_1, \cdots , z_r$ and for all $t$ we have
\begin{align}
 \int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r \frac{ 1_{ (x_i,y_i) } \in t(z_i) }{ vol(t(z_i)) }    d x_i d y_i =  0
\end{align}
For any  sequence of rectangles  $( (a_i,b_i) \times (c_i,d_i) )_{i=1}^{n}$ there exists $t \in T$ such that  $t(z_i) = (a_i,b_i) \times (c_i,d_i)$. We substitute
\begin{align}
\int_{a_1}^{b_1} \int_{c_1}^{d_2} \cdots \int_{a_r}^{b_r} \int_{c_r}^{d_r} h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r}  dx_i = 0. 
\end{align}
This implies that for any fixed  $z_1, \cdots , z_r$ function  $h(\cdot,\cdot,z_1, \cdots ,z_r)$ is zero almost everywhere, therefore  $h$ is zero almost everywhere.
\end{proof}







Let   $\mathbf X = (X_1,\cdots, X_n)$ be a sequence of iid random variables; for a measure $p$ we will write $\mathbf X \sim p$ to indicate that each $X_i$ is distributed according to  $P$.  Null and alternative hypothesis are a two, disjoint sets of probability measures, $\mathcal{P}$ and $\mathcal{Q}$ respectively. Here we assume that the alternative hypothesis is complement of the null hypothesis,  $\mathcal Q = \mathcal{P}^c$.  A statistical test is an integrable, symmetric function form  $\mathbf X$ to $\{0,1\}$. We say that the test $\psi$ accepts a null hypothesis if $\psi(\mathbf X)=0$, otherwise we say that test rejects of the null hypothesis. 

For a statistical test $\psi$, type one error is probability that the test rejects the null hypothesis 
\[
 H(\mathcal{P}) =  \sup_{\mathbf{X} \sim p \in \mathcal{P} }   p( \psi(\mathbf X) =1 )
\]
and maximal power of the test is 
\[
H(\mathcal{Q}) \sup_{p \in \mathcal{Q} }  = p( \psi(\mathbf X) =1 ).
\]
Type one error and maximal power is just $H$  calculated on two different sets,  $\mathcal{P}$ and $\mathcal{Q}$. The minimal requirement for a statistical test is that for any fixed level $\alpha$, $H(\mathcal{P}) \leq \alpha$ and $H(\mathcal{P}^c) > \alpha$, which amounts to saying that type one error is controlled on the level alpha and there exists an distribution form the alternative for which test power is large. If a statistical test satisfies this condition, we say it is OK. 
 
In order to continue we need to restrict further the class of out tests and measures bla bla 

\begin{statement}
If $\psi$ is $\alpha$ minimal test, then there exist a continuous, bounded, symmetric function $h: R^{3n} \to R$ such that for all $p \in \mathcal{P}$ 
\[
 \int_{R^{3n}} h(x) dp(x)  \leq 0
\]
\end{statement}

\begin{proof}
 Put $h(x) = 1_\{ \psi(x) =1 \} - \alpha$. 
 \[
  \int_{R^{3n}} h(x) dp(x) = p( \psi(x) =1) - \alpha \leq 0.
 \]
$h$ is symmetric since $\psi$ is symmetric. 
\end{proof}

We consider set of all probability measures, with densities that allow factorization $p(x,y,z)p(z) = p(x,z)p(y,z)$, and we call this set $C$. Note that set $G \subset C$. But if integral of $h$ is zero on $G$ then its zero on on set product measures on $p \times p_1 \times \cdot \times p_r$ bu the Theorem 2 (since $h$ is zero!).  integral $h$ is continuous linear operator on measures with respect to weak topology. This means that $span(G)$ is dense in probability measures. Integral of $h$ naturally extends itself to the $span(G)$ and $span(C)$. Due to denseness  

Let $E$ be a space of signed measures, supported on a hypercube centered at zero, with bounded densities. $E$ is a real vector space (with operations defined on densities). Let $F$ be a span of $\mathcal P \cup E$. Let $K$ be set of positive measures in $V$. $K$ is a convex cone in $V$, since $K$ is closed under linear combinations with positive coefficients.

It is sufficient to show that there is no minimal test for conditional independence for probability measures form $K$. 

$F = span(G)$, let $p$ be not conditionally independent measure, not in $E$ and let $cone(G)$. 
Let $E = span(F \cup p)$ and $K= cone(G \cup a)$. Intersession of $F$ and $K$ is $cone(G)$ so functional is negative on it. 
For any $y \in E$ there exits $x \in F$, st $y-x \in K$.  Indeed $y = f + \alpha a$, where $f \in F$ and $a \in K$.





\begin{Theorem}
 Let $h$ be a measurable, symmetric function such that for $p \in \mathcal G$ 
 \[
 E h(X_1, \cdots , X_n) \leq 0.
\]
where $X_i$ are iid from $p$. For all $q \in K$
\[
 E h(X_1, \cdots , X_n) \leq 0.
\]
where $X_i$ are iid from $q$
\end{Theorem}

\begin{proof}
 
 For any signed measure  $p \times ... p$ we define continuous linear map $\phi$
 \[
  \phi(p) =  E h(X_1, \cdots , X_n)
 \]
where $X_i \sim p$ are iid random variables. If $p \in G \cup K$ then, by assumption,
\[
 p = \sum_{i=1}^N \alpha_i p_i \times ... p_i
\]
where $p_i$ are indicator functions 
First alpha must sum to 1, otherwise p is not in K. Next suppose $\alpha_1$ is negative. Suppose for some $z$, there exist $x,y$ for which 
$p_1(x,y,z) >0 $ but other $p_i$ are zero - zonk density is negative. It might be that that support of $p_1$ is covered by other $p_j$  



\[
  \phi(p) \leq 0.
\]
$F$ contains uniform distribution over the hypercube -- we will denote it as $u$. Any element  $e \in E$ can be expressed as a sum of $x \in K$ and $\alpha u$ for real alpha; this follows from the fact that elements  $ e \in E$ are bounded and therefore can be shifted by $\alpha u$ so that $e + \alpha u$ are positive. We are in position to use Riesz extension theorem and extend $\phi$ to $\psi$ defined the whole cone $K$, such that 
\[
 \phi |_{F} = \psi \text{ and } \psi(p) \leq 0 \text{ for} p \in K.
\]
We will now show that this extension is unique. Suppose there exist some other function $\kappa$ such that $\kappa |_{F} = \psi$ and $\kappa$ is given by an integral of some symmetric function $h'$
 \[
  \kappa(p) =  E h'(X_1, \cdots , X_n).
 \]
Then $\kappa - \psi |_{\mathcal{P}} = 0$ and $\kappa - \psi$ is given by a integral with respect to a symmetric function. By Theorem \ref{th:1} $g$ must be equal to zero everywhere and so $\kappa =0$.   
\end{proof}

We have shown that for if there exist $\psi$ such that 
$$P(\psi(X_1,\cdots, X_n) = 1) \leq \alpha$$,
for all $p \in \mathcal{P}$ then 
\[
 P(\psi(X_1,\cdots, X_n) = 1) \leq \alpha
\]
for all $p \in K$, so there exist no distribution for which type two error is small.  










\paragraph{Old junk}






\paragraph{conditional covariance}
The same argument as above works for the conditional covariance. The family $\mathcal G$ used in the proof of the Theorem \ref{th:1} is a family of densities for which $E(XY|Z)=E(X|Z)E(Y|Z)$. 

\paragraph{Literature overview}

\paragraph{regression like approach}
This one \cite{song2007testing} requires strong assumptions on conditional  expected values convergence in the Theorem one, I think. \cite{su2008nonparametric} clear form their th 3.1 (use kde as well). same \cite{fukumizu2007kernel,zhang2012kernel}.
\cite{huang2010testing}.


\paragraph{mixing events with sigma fields}
\cite{gyorfi2012strongly}


\cite{linton1996conditional} 'In particular, a smoothing based test would not be able to detect
alternatives at distance $n^{0.5}$ from the null detected by parametric and
some nonparametric test statistics.
Our strategy to avoid this dimensionality problem is to verify the
relationship (5) over subsets (with positive Lebesgue measure) in the
support of Y , X , Z rather than at individual values, thereby replacing
densities with distribution functions. Expression (5) is extended to
$P (C )P (A,B, C ) = P (A,C )P (B,C )$,
for all subsets A, B in the support of Y , X , and C a subset in the open
support of Z.



\paragraph{Other approach that assumes continuity also partially works}

Following \cite{bergsma2014consistent} we define a discrepancy function
\begin{equation}
a(p_1,p_2,p_4,p_4) = |p_1-p_2| -|p_1-p_3|-|p_2-p_4|+|p_4-p_3|.
\end{equation}
and 
\begin{equation}
\tau = Ea(X_1,X_2,X_3,X_4)a(Y_1,Y_2,Y_3,Y_4).
\end{equation}
which is useful in characterizing independence, as seen by 
\begin{Theorem}[\cite{bergsma2014consistent} ]
\label{th:tau_star}
Let $P$ be a distribution on $R^2$, with a density.  $\tau = 0$ if and only if $X_i$ and $Y_i$ independent for  $(X_i,Y_i) \sim P$. 
\end{Theorem}

We adopt $\tau$ for the conditional independence testing. Given samples $(X_i,Y_i,Z_i)$ we define  $X^{i},Y^{i},Z^{i}$ to be a sequence of observations ordered increasingly by $Z_i$ -- we will call these the order statistics with respect to $Z$ . Let
 \[ 
\tau_i = sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})).  
 \]
We assume that if $Z$ is close to $Z'$ then  distribution $X,Y|Z$ is close to  $X',Y'|Z'$ in some sense; specifically  there exist a number $p$,  random variables  $\bar X'$, $\bar Y'$ such that $P( |X'- \bar X'|>\epsilon,|Y'- \bar Y'|>\epsilon  ) \leq P(|Z-Z'|^p>\epsilon)$ and $\bar X', \bar Y'$ are distributed as $X,Y$. For now we will call such distributions 'p-good'. We will show that, under this assumption, $\tau_i$'s will asymptotically behave like $\tau$.   

We adopt this particular test since it is distribution free, and the test static has easy distribution under the null hypothesis. For example, should we use measure like person correlation, we would have to estimate the conditional variance to calculate the correlation. In contrast, the distribution of $\tau$ has aways constant variance, equal to one, and under the null hypothesis probability of one and minus one are the same.  We start with an auxiliary facts and lemma
\begin{statement}
\label{lem:err}
For any numbers $(z_i,z_i')$, 
\[
| a(z_1,z_2,z_4,z_4) -  a(z_1',z_2',z_4',z_4')| \leq 2\sum_{i=1}^4 |z_i-z_i'|
\]
\end{statement}


\begin{lemma}
\label{lem:bnd}
Let $(A_n,B_n,E_n)$ be a sequence of random variables such that  $|A_n-B_n| \leq |E_n|$. For any sequence $\delta_n$
\[
\left | E  sgn(A_n) - sgn(B_n) \right | \leq  2P(|E_n|>\delta_n) + 2 P(|E_n| < \delta_n ,|A_n| < \delta_n)
\]
\end{lemma} 
\begin{proof}
We will bound the difference $E sgn(A_n) - sgn(B_n)$,
\begin{align}
E&  sgn(A_n) - sgn(B_n)= \\
&E \mathbf  1 \{ |E_n| < \delta_n ) , |B_n| > \delta_n \} 0 +\\
&E \mathbf 1 \{ |E_n| < \delta_n)  , |B_n| < \delta_n\}  (sgn(A_n) - sgn(B_n)) + \\
&E \mathbf 1 \{ |E_n| > \delta_n  \}  (sgn(A_n) - sgn(B_n)).  
 \end{align}
The last two terms can be bounded
 \begin{align}
&   |E\mathbf 1 \{ |E_n| > \delta_n   \}  sgn(A_n) - sgn(B_n)| \leq 2 P( |E_n|>\delta_n) , \\
&   |E \mathbf 1 \{ |E_n| < \delta_n   , |B_n| < \delta  \}  sgn(A_n) - sgn(B_n) | \leq 2 P(|E_n| < \delta_n ,|A_n| < \delta_n) . 
\end{align}
\end{proof}




\begin{statement}
\label{lemma:keyLemma}
 for any real numbers $a,b,c,d$, 
 \[
  2(ab -cd) = (a-c)(b+d) + (a+c)(b-d). 
 \]
\end{statement}

% \begin{definition}
% Let  $X_i,Y_i,Z_i$ be a sequence such that $P( |X_i-X_j|>\epsilon,|Y_i-Y_j|>\epsilon  ) \leq P(|Z_i-Z_j|^p>\epsilon)$. 
% Let $W^{i},Z^{i}$ be order statistics by $Z$. We call them p-good if for any $i$ there exist a sequence of independent random variables $\bar W^i$ such that 
% for any $i$, and $j \in {0,1,2,3}$
% \[
%  P( |\bar W^{i+j} - W^{i+j}| > a) \leq P( |Z^{i} - Z^{i+j}|^p >a),
% \]
% ,$\bar W^{i+j}$ is distributed as $W^{i}$. 
% \end{definition}

\begin{Theorem}
If $X_i,Y_i,Z_i$ is are 'p-good' and $P( |X| <t) <Ct$, $P( |Y| <t) <Ct$ for small $t$ and $Z$ is uniform over $[0,1]$, then
\[
 \frac{1}{\sqrt n} \sum_{j=0;j=+4;n} \tau_j   
\]
converges to standard normal distribution  if and only iff  $X_i,Y_i,Z_i$ are conditionally independent.
\end{Theorem}
\begin{proof}
Since $Var(\tau_j)=1$, the sum 
\[
 \frac{1}{\sqrt {n/4}} \sum_{i=0;i=+4;n} (\tau_i - E \tau_i)   
\]
converges to standard  normal distribution. It is sufficient to prove that $\frac{1} {\sqrt n} \sum_{i}^{n} E \tau_i=0$ converges to zero if and only if  null hypothesis holds. For any  $i=4k$, where $k$ is natural, and $j \in {0,1,2,3}$ let $\bar X_{i+j}$ and $\bar Y_{i+j}$ be independent random variables provided by the 'p-good' assumption i.e. 
\[
 P( |X^{i+j}- \bar X^{i+j}|>\epsilon,|Y^{i+j}- \bar Y^{i+j}|>\epsilon  ) \leq P(|Z^i-Z^{i+j}|^p>\epsilon),
\]
and $\bar X^{i+j},\bar Y^{i+j}$ are distributed as $X^i,Y^i$. 
%%TODO give a better definition above to make sure all is independent.

Define 
\[
 \bar \tau_j = sgn(a(\bar X^{j},\bar X^{j+1},\bar X^{j+2}, \bar X^{j+3})) sgn(a(\bar Y^{j},\bar Y^{j+1},\bar Y^{j+2},\bar Y^{j+3}))
\]
By \ref{th:tau_star} $E \bar \tau_j=0$ if and only if  $\bar X_i , \bar Y_i$ are independent. Therefore, under the null hypothesis it is sufficient to show  that $ E (\bar \tau_j - \tau_j) = O(1/ \sqrt n)$. Indeed, 
\[
 \frac{1} {\sqrt n} \sum_{i}^{n} E (\tau_i - \bar \tau_j +\bar \tau_j) = \frac{1} {\sqrt n} \sum_{i}^{n} O(1/ \sqrt n) +0 \to 0.
\]
By the observation \ref{lemma:keyLemma} we see it is sufficient to prove that  
\begin{align}
   E sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) - sgn(a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})) \to a_i  \\
  \sqrt n  E sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})) - sgn(a(\bar Y^{i},\bar Y^{i+1},\bar Y^{i+2}, \bar Y^{i+3})) \to a_i
\end{align}
Without loss of generality we can consider only $X$'s. Let $A_n = a(X^{i},X^{i+1},X^{i+2},X^{i+3})$, $B_n = a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})$ . By the lemma \ref{lem:err} $ A_n-B_n \leq E_n = 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}|$. By the lemma  \ref{lem:bnd} it is enough to show that  $P( E_n > \delta_n ) = O(1/ \sqrt n)$ and $P( |B_n| < \delta_n )=O(1/ \sqrt n)$ for some  $\delta_n$. We choose $\delta_n = \frac{C} {n}$. For $B_n$ we have 
\[
 P( |B_n| < \frac{1}{\sqrt n} ) \leq \frac{C}{\sqrt n}. 
\]
By p-good assumption
\begin{align}
 P(|E_i| > \frac{1}{\sqrt n} ) = P( 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}| > \frac{1}{\sqrt n} ) \leq \\
 P( 8|Z^i - Z^{i+4}|^p > \frac{1}{\sqrt n} )   =  P(  8|Z^i - Z^{i+4}| > n^{-1/2p} ).
\end{align}
By (reference)  $D_{n,i} = |Z^i - Z^{i+4}|$ is distributed as $Beta(4,n-3)$.  One can show that for $p>0.5$ prove
\[
 \sqrt n P( n D_{n,i} > n^{-1/2p+1}) \to 0 
\]
Simple way to see it is true is to  notice that $n D(n,i)$ coverages to gamma(k,1) and we need $-1/2p+1 >0 $ so $n^{-1/2p+1}$ grows (any polynomial growth is OK since gamma has heavy tail).  $-1/2p+1 >0$ is equivalent to $p>0.5$. This is not proof since we used limit twice.
\end{proof}

How to construct $\bar X^{i+j}$ ? We set $\bar X^{i+j} =  F^i(F^{i+j}(X^{i+j})$. Let 
\[
 P( |\bar X^{i+j} -X^{i+j}  |>a) = P( |F^i(F^{i+j}(X^{i+j}) -X^{i+j}  |>a) 
\]
clearly we need to make some assumptions  of $F$, for example of the following should do:  if $|z-z'| \leq a $
\[
 |x-x'| 
\]






% 
% \bibliographystyle{plain}
% \bibliography{acc}


\end{document}


