% !TEX program = xelatex
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\kacper}[1]{ \bf  { \color{Orchid}{Kc: #1}}  }

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{statement}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem{test}{Test}
\newtheorem{proposition}{Proposition}

\newtheorem*{remark}{Remark}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

\title{}
\author{KC}
\date{}



\begin{document}

\maketitle

\section{}


\paragraph{Fixing the proof of testability.}
\cite{bergsma2004testing} introduces a concept of testability in attempt to prove that conditional independence is not testable, however there is a mistake in the proof.



\paragraph{Alternative proof.}
All the equations (for functions) in the following note  hold almost everywhere, unless explicitly specified otherwise.
\begin{definition}
 A parameter  $\theta$ of a probability measure $p \in \mathcal P$, where $\mathcal P$ is a set of probability measures, is a function $\mathcal P  \to  \mathbf R$.   
\end{definition}


% According to \cite{bergsma2004testing} this definition is by Hoeffding (to be verified). 
\begin{definition}[Hoeffding]
A parameter  $\theta$ is called estimable of degree $r$, if $r$ is the smallest number for which there is a bounded function $h : \mathbf R^{r} \to \mathbf R$ such that, for all $p \in \mathcal P$, $ \theta(p) = Eh(X_1, \cdots , X_r)$ (exactly),  if $X_i$ are iid random variables sampled according to $p$.
\end{definition}

\cite{bergsma2004testing} introduces a concept of testability which is a 'syntactic sugar' on the concept of estimable parameter.

\begin{definition}[testability, \cite{bergsma2004testing}]
 Let $\mathcal{P}$ be a set of probability measures and $\mathcal{P}'$ its complement. Let $\Theta$ be a family of  estimable parameters which are zero on all elements of $\mathcal{P}$.  $\mathcal{P}$ is non-testable if and only if $\Theta$ contains only $0$ function. Otherwise $\mathcal{P}$ is testable. 
\end{definition}



Non-testability implies that a statistical test based on $U$-statistics can not be conducted, which does not rule out, as remarked by G. Peters, possibility of creating tested based on some other estimators e.g. M-estimator. The lemmas we give next are a used in the proof of the Theorem  \ref{th:1}, stating non-testability of conditional independence. 

\begin{lemma}
\label{lem:aha}
 The only polynomial of $m$ variables that is zero on the $m$-dimensional simplex is $\sum_{i=1}^m x_i-1$ and $0$.  
\end{lemma}
\begin{proof}
Consider restriction of the polynomial on the $R^m$ space  the affine space $\sum_{i=1}^m x_i=1$  -- this restriction is a polynomial of $m-1$ variables as seen by substitution $x_1 = 1 -\sum_{i=2}^m x_i$. This restriction zero is on the simplex $0 \leq x_i \leq 1$, therefore it must be zero on the whole affine space  $\sum_{i=1}^m x_i=1$. This is due to the fact that set of zeros of the polynomial is nowhere dense on its domain. The only polynomials that are zero on a set $\sum_{i=1}^m x_i=1$ are $\sum_{i=1}^m x_i-1$ and $0$.   
\end{proof}



\paragraph{alternative formulation for $L_1$}


\begin{lemma}
\label{the:Lemma}
Let $h: R^r \to R$ be a bounded function, symmetric in its arguments, i.e. for any permutation $\pi$ of set $(1,\cdots,r)$,
$$
h(x_{\pi(1)},\cdots,x_{\pi(1)}) = h(x_1,\cdots,x_r).   
$$
If for any sequence of iid random variables $(Z_1,\cdots,Z_r)$ the expected value of $h$ is zero, i.e.
$$
\int h(z_1,\cdots,z_r) \prod_{i=1}^n p_i(z_i) d z_i = 0,
$$
then $h$ is equal to zero.
\end{lemma}

\begin{proof}
 Take $f(z) = \sum_{i=1}^{r} \alpha_{i} f(z)$ and plug it to integral. The result is polynomial in $\alpha$'s which coefficients are given by
 \[
  \beta_{i_1,...,i_r} = \int \alpha_{i_1} p_{i_1} \cdots \alpha_{i_r} p_{i_r} h(z_1,\cdots,z_r)
 \]
Note that $\beta$ is invariant to permutation of indexes $i_1,...,i_r$. Also all $\beta$'s are equal to zero since the polynomial is equal to zero. 
Consider coefficient $b_{1,\cdots, n} = 0$, and put $p_i = 1{[a_i,b_i]} $
\begin{align}
 \int \alpha_{1} p_{1}(z_1) ...\alpha_{r} p_{r}(z_r) p(z_r)\cdots \alpha_{i_r} h(z_1,\cdots,z_r) =\
 \alpha_{1}  ...\alpha_{r}  \int_{a_1}^{b_1} \cdots  \int_{a_r}^{b_r}  h(z_1,\cdots,z_r) 
\end{align}

Which means that $h$ is zero.
 
\end{proof}




\begin{Theorem}
\label{th:1}
Let $\mathcal P = \{ p(x,y,z) \in L_1(R^3) | p(x,y,z)p(z) = p(x,z)p(y,z) \}$. If $f : R^{3n} \to R$ is such that 
\[
 \int_{R^{3n}} h(x_1,y_1,z_1,\cdots,x_n,y_n,z_n) \prod_{i=1}^n p(x_i,y_i,z_i) d x_i d y_i d z_i = 0
\]
on $p \in \mathcal P$, then $h=0$ almost everywhere.
\end{Theorem}

\begin{proof}
Assume to the contrary that such an $h$ exists. By the Fubini thorem  
\begin{align}
 \int& \left(\int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r p(x_i,z_i)p(y_i,z_i)/p(z_i)^2 dx_i dy_i  \right)  \prod_{i=1}^r p(z_i)dz_i
\end{align}
Let $\mathcal I = \{ p'(x,y,z) \in \mathcal P | p(x,z)/p(z) = p'(x,z)/p'(z) \text{ and }(y,z)/p(z) = p'(y,z)/p'(z)$. For any $g \in L_1(R)$ there exists $p' \in I$ such that $\int p'(x,y,z) dx dz = f(z)$. We put $p' = p(x,z)p(y,z)/p(z)^2 f(z)$. We check 
\begin{align}
 p'(x,z) = p(x,z)/p(z) f(z)
 p'(y,z) = p(y,z)/p(z) f(z)
 p'(z) = f(z)
\end{align}



Denote $g_{I}( z_1,\cdots,z_r) = \int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r p(x_i,z_i)p(y_i,z_i)/p(z_i)^2 dx_i dy_i$. We write $I$ to remember that $g$ is same for elements in $I$. We show that $g_{I}$ is symmetric with respect to its arguments. 
\begin{align}
&g_{I}(z_{\pi(1)}, \cdots,z_{\pi(r)} )  = \\ 
& \int h(x_1,y_1,z_1,\cdots, x_r,y_r,z_r) \prod_{i=1}^r p(x_i,z_i)p(y_i,z_i)/p(z_i)^2 dx_i dy_i = \\
&g_{I}(z_1, \cdots,z_r ). 
\end{align}

By Lemma \ref{the:LemmaAlt}, $g_{I}=0$ almost everywhere. 
Let $I$ be a set of all intervals on the real line. Let  $T$ be all family of all functions $R \to I^2$. Consider a family of densities $\mathcal G$
\[
 \mathcal G = \bigg \{ 1\{ (x,y) \in t(z) \}  \frac{1}{vol(t(z))} p(z) | t \in T \bigg \}
\]
where $f(z)$ is some $L_1$ function. We have shown that for all $t \in T$ $g_t=0$ almost everywhere.  This means that 
\begin{align}
&\int h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r} 1\{ (x,y) \in t(z) \}  \frac{1}{vol(t(z))} dx_i dy_i =\\ 
&\int_{a_1}^{b_1} \int_{c_1}^{d_2} \cdots \int_{a_r}^{b_r} \int_{c_r}^{d_r} h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r}  \frac{dx_i dy_i}{(b_i - a_i)(d_i-c_i) }. \\   
\end{align}
where $(a_i,b_i) \times (c_i,d_i) = t(z_i)$. 
By equation \ref{ref:eqg} we have 
\begin{align}
 \int_{a_1}^{b_1} \int_{c_1}^{d_2} \cdots \int_{a_r}^{b_r} \int_{c_r}^{d_r}& h(x_1,y_1,z_1, \cdots, x_r,y_r,z_r) \prod_{i=1}^{r}  dx_i dy_i = 0.
\end{align}
Since for any  sequence $ (a_i,b_i)\times (c_i,d_i) $, there exists  $t: t(z_i) = (a_i,b_i)\times (c_i,d_i) $, $h=0$. 
\end{proof}
















\paragraph{No tests for CI}

Let   $\mathbf X = (X_1,\cdots, X_n)$ be a sequence of iid random variables; for a measure $p$ we will write $\mathbf X \sim p$ to indicate that each $X_i$ is distributed according to  $P$.  Null and alternative hypothesis are a two, disjoint sets of probability measures, $\mathcal{P}$ and $\mathcal{Q}$ respectively. Here we assume that the alternative hypothesis is complement of the null hypothesis,  $\mathcal Q = \mathcal{P}^c$.  A statistical test is a measurable, symmetric function form  $\mathbf X$ to $\{0,1\}$. We say that the test $\psi$ accepts a null hypothesis if $\psi(\mathbf X)=0$, otherwise we say that test rejects of the null hypothesis. 

For a statistical test $\psi$, type one error is probability that the test reject the null hypothesis 
\[
 H(\mathcal{P}) =  \sup_{\mathbf{X} \sim p \in \mathcal{P} }   P( \psi(\mathbf X) =1 )
\]
and maximal power of the test is 
\[
H(\mathcal{Q}) \sup_{p \in \mathcal{Q} }   p( \psi(\mathbf X) =1 ).
\]
 Type one error and maximal power is just $H$  calculated on two different sets,  $\mathcal{P}$ and $\mathcal{Q}$. The minimal requirement for a statistical test is that for any fixed level $\alpha$, $H(\mathcal{P}) \leq \alpha$ and $H(\mathcal{P}^c) > \alpha$, which amounts to saying that type one error is controlled on the level alpha and there exists an distribution form the alternative for which test power is large. If a statistical test satisfies this condition, we say its minimal. 

\begin{statement}
If $\psi$ is $\alpha$ minimal test, then there exist a measurable, bounded, symmetric function $h: R^{3n} \to R$ such that for all $p \in \mathcal{P}$ 
\[
 \int_{R^{3n}} h(x) dp(x)  \leq 0
\]
\end{statement}

\begin{proof}
 Put $h(x) = 1_\{ \psi(x) =1 \} - \alpha$. 
 \[
  \int_{R^{3n}} h(x) dp(x) = p( \psi(x) =1) - \alpha \leq 0.
 \]
$h$ is symmetric since $\psi$ is symmetric. 
\end{proof}



defined on  $R^{3n}$, where each $X_i$ is a three-dimensional real valued random vector. This notation is useful for talking about conditional independence - each $X_i$ is a triple of random variables $(X_{i,1},X_{i,2},X_{i,3}$ such that $X_{i,1}$ is conditionally independent of $X_{i,2}$ given $X_{i,3}$.


The minimal requirement for a test is that type one error is controlled at some level $\alpha$ while there exist a distribution form the alternative for which type two error is small. we call such a test a minimal test.  Let $\mathcal P$ be a family of distributions on $R^3$, with densities, that are conditionally independent. If there exists the minimal test for conditional independence then there exist a measurable, symmetric function $h$ such that for all $X_i \sim P$, where $P \in \mathcal P$
\[
 E h(X_1, \cdots , X_n) \leq 0.
\]
$h$ is given by $1_\{ \psi(X_1, \cdots, X_n) =1 \} - \alpha$.  

Let $E$ be a space of signed measures, supported on a hypercube centered at zero, with bounded densities. $E$ is a real vector space (with operations defined on densities). Let $F$ be a span of $\mathcal P \cup E$. Let $K$ be set of positive measures in $V$. $K$ is a convex cone in $V$, since $K$ is closed under linear combinations with positive coefficients.

It is sufficient to show that there is no minimal test for conditional independence for probability measures form $K$. 



\begin{Theorem}
 Let $h$ be a measurable, symmetric function such that for $p \in \mathcal P$ 
 \[
 E h(X_1, \cdots , X_n) \leq 0.
\]
where $X_i$ are iid from $p$. For all $q \in K$
\[
 E h(X_1, \cdots , X_n) \leq 0.
\]
where $X_i$ are iid from $q$
\end{Theorem}

\begin{proof}
 
 For any signed measure  $p$ we define linear map $\phi$
 \[
  \phi(p) =  E h(X_1, \cdots , X_n)
 \]
where $X_i \sim p$ are iid random variables. If $p \in F \cup K$ then, by assumption, 
\[
  \phi(p) \leq 0.
\]
$F$ contains uniform distribution over the hypercube -- we will denote it as $u$. Any element  $e \in E$ can be expressed as a sum of $x \in K$ and $\alpha u$ for real alpha; this follows from the fact that elements  $ e \in E$ are bounded and therefore can be shifted by $\alpha u$ so that $e + \alpha u$ are positive. We are in position to use Riesz extension theorem and extend $\phi$ to $\psi$ defined the whole cone $K$, such that 
\[
 \phi |_{F} = \psi \text{ and } \psi(p) \leq 0 \text{ for} p \in K.
\]
We will now show that this extension is unique. Suppose there exist some other function $\kappa$ such that $\kappa |_{F} = \psi$ and $\kappa$ is given by an integral of some symmetric function $h'$
 \[
  \kappa(p) =  E h'(X_1, \cdots , X_n).
 \]
Then $\kappa - \psi |_{\mathcal{P}} = 0$ and $\kappa - \psi$ is given by a integral with respect to a symmetric function. By Theorem \ref{th:1} $g$ must be equal to zero everywhere and so $\kappa =0$.   
\end{proof}

We have shown that for if there exist $\psi$ such that 
$$P(\psi(X_1,\cdots, X_n) = 1) \leq \alpha$$,
for all $p \in \mathcal{P}$ then 
\[
 P(\psi(X_1,\cdots, X_n) = 1) \leq \alpha
\]
for all $p \in K$, so there exist no distribution for which type two error is small.  



% 
% \paragraph{conditional covariance}
% The same argument as above works for the conditional covariance. The family $\mathcal G$ used in the proof of the Theorem \ref{th:1} is a family of densities for which $E(XY|Z)=E(X|Z)E(Y|Z)$. 
% 
% \paragraph{Literature overview}
% 
% \paragraph{regression like approach}
% This one \cite{song2007testing} requires strong assumptions on conditional  expected values convergence in the Theorem one, I think. \cite{su2008nonparametric} clear form their th 3.1 (use kde as well). same \cite{fukumizu2007kernel,zhang2012kernel}.
% \cite{huang2010testing}.
% 
% 
% \paragraph{mixing events with sigma fields}
% \cite{gyorfi2012strongly}
% 
% 
% \cite{linton1996conditional} 'In particular, a smoothing based test would not be able to detect
% alternatives at distance $n^{0.5}$ from the null detected by parametric and
% some nonparametric test statistics.
% Our strategy to avoid this dimensionality problem is to verify the
% relationship (5) over subsets (with positive Lebesgue measure) in the
% support of Y , X , Z rather than at individual values, thereby replacing
% densities with distribution functions. Expression (5) is extended to
% $P (C )P (A,B, C ) = P (A,C )P (B,C )$,
% for all subsets A, B in the support of Y , X , and C a subset in the open
% support of Z.
% 
% 
% 
% \paragraph{Other approach that assumes continuity also partially works}
% 
% Following \cite{bergsma2014consistent} we define a discrepancy function
% \begin{equation}
% a(p_1,p_2,p_4,p_4) = |p_1-p_2| -|p_1-p_3|-|p_2-p_4|+|p_4-p_3|.
% \end{equation}
% and 
% \begin{equation}
% \tau = Ea(X_1,X_2,X_3,X_4)a(Y_1,Y_2,Y_3,Y_4).
% \end{equation}
% which is useful in characterizing independence, as seen by 
% \begin{Theorem}[\cite{bergsma2014consistent} ]
% \label{th:tau_star}
% Let $P$ be a distribution on $R^2$, with a density.  $\tau = 0$ if and only if $X_i$ and $Y_i$ independent for  $(X_i,Y_i) \sim P$. 
% \end{Theorem}
% 
% We adopt $\tau$ for the conditional independence testing. Given samples $(X_i,Y_i,Z_i)$ we define  $X^{i},Y^{i},Z^{i}$ to be a sequence of observations ordered increasingly by $Z_i$ -- we will call these the order statistics with respect to $Z$ . Let
%  \[ 
% \tau_i = sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})).  
%  \]
% We assume that if $Z$ is close to $Z'$ then  distribution $X,Y|Z$ is close to  $X',Y'|Z'$ in some sense; specifically  there exist a number $p$,  random variables  $\bar X'$, $\bar Y'$ such that $P( |X'- \bar X'|>\epsilon,|Y'- \bar Y'|>\epsilon  ) \leq P(|Z-Z'|^p>\epsilon)$ and $\bar X', \bar Y'$ are distributed as $X,Y$. For now we will call such distributions 'p-good'. We will show that, under this assumption, $\tau_i$'s will asymptotically behave like $\tau$.   
% 
% We adopt this particular test since it is distribution free, and the test static has easy distribution under the null hypothesis. For example, should we use measure like person correlation, we would have to estimate the conditional variance to calculate the correlation. In contrast, the distribution of $\tau$ has aways constant variance, equal to one, and under the null hypothesis probability of one and minus one are the same.  We start with an auxiliary facts and lemma
% \begin{statement}
% \label{lem:err}
% For any numbers $(z_i,z_i')$, 
% \[
% | a(z_1,z_2,z_4,z_4) -  a(z_1',z_2',z_4',z_4')| \leq 2\sum_{i=1}^4 |z_i-z_i'|
% \]
% \end{statement}
% 
% 
% \begin{lemma}
% \label{lem:bnd}
% Let $(A_n,B_n,E_n)$ be a sequence of random variables such that  $|A_n-B_n| \leq |E_n|$. For any sequence $\delta_n$
% \[
% \left | E  sgn(A_n) - sgn(B_n) \right | \leq  2P(|E_n|>\delta_n) + 2 P(|E_n| < \delta_n ,|A_n| < \delta_n)
% \]
% \end{lemma} 
% \begin{proof}
% We will bound the difference $E sgn(A_n) - sgn(B_n)$,
% \begin{align}
% E&  sgn(A_n) - sgn(B_n)= \\
% &E \mathbf  1 \{ |E_n| < \delta_n ) , |B_n| > \delta_n \} 0 +\\
% &E \mathbf 1 \{ |E_n| < \delta_n)  , |B_n| < \delta_n\}  (sgn(A_n) - sgn(B_n)) + \\
% &E \mathbf 1 \{ |E_n| > \delta_n  \}  (sgn(A_n) - sgn(B_n)).  
%  \end{align}
% The last two terms can be bounded
%  \begin{align}
% &   |E\mathbf 1 \{ |E_n| > \delta_n   \}  sgn(A_n) - sgn(B_n)| \leq 2 P( |E_n|>\delta_n) , \\
% &   |E \mathbf 1 \{ |E_n| < \delta_n   , |B_n| < \delta  \}  sgn(A_n) - sgn(B_n) | \leq 2 P(|E_n| < \delta_n ,|A_n| < \delta_n) . 
% \end{align}
% \end{proof}
% 
% 
% 
% 
% \begin{statement}
% \label{lemma:keyLemma}
%  for any real numbers $a,b,c,d$, 
%  \[
%   2(ab -cd) = (a-c)(b+d) + (a+c)(b-d). 
%  \]
% \end{statement}
% 
% % \begin{definition}
% % Let  $X_i,Y_i,Z_i$ be a sequence such that $P( |X_i-X_j|>\epsilon,|Y_i-Y_j|>\epsilon  ) \leq P(|Z_i-Z_j|^p>\epsilon)$. 
% % Let $W^{i},Z^{i}$ be order statistics by $Z$. We call them p-good if for any $i$ there exist a sequence of independent random variables $\bar W^i$ such that 
% % for any $i$, and $j \in {0,1,2,3}$
% % \[
% %  P( |\bar W^{i+j} - W^{i+j}| > a) \leq P( |Z^{i} - Z^{i+j}|^p >a),
% % \]
% % ,$\bar W^{i+j}$ is distributed as $W^{i}$. 
% % \end{definition}
% 
% \begin{Theorem}
% If $X_i,Y_i,Z_i$ is are 'p-good' and $P( |X| <t) <Ct$, $P( |Y| <t) <Ct$ for small $t$ and $Z$ is uniform over $[0,1]$, then
% \[
%  \frac{1}{\sqrt n} \sum_{j=0;j=+4;n} \tau_j   
% \]
% converges to standard normal distribution  if and only iff  $X_i,Y_i,Z_i$ are conditionally independent.
% \end{Theorem}
% \begin{proof}
% Since $Var(\tau_j)=1$, the sum 
% \[
%  \frac{1}{\sqrt {n/4}} \sum_{i=0;i=+4;n} (\tau_i - E \tau_i)   
% \]
% converges to standard  normal distribution. It is sufficient to prove that $\frac{1} {\sqrt n} \sum_{i}^{n} E \tau_i=0$ converges to zero if and only if  null hypothesis holds. For any  $i=4k$, where $k$ is natural, and $j \in {0,1,2,3}$ let $\bar X_{i+j}$ and $\bar Y_{i+j}$ be independent random variables provided by the 'p-good' assumption i.e. 
% \[
%  P( |X^{i+j}- \bar X^{i+j}|>\epsilon,|Y^{i+j}- \bar Y^{i+j}|>\epsilon  ) \leq P(|Z^i-Z^{i+j}|^p>\epsilon),
% \]
% and $\bar X^{i+j},\bar Y^{i+j}$ are distributed as $X^i,Y^i$. 
% %%TODO give a better definition above to make sure all is independent.
% 
% Define 
% \[
%  \bar \tau_j = sgn(a(\bar X^{j},\bar X^{j+1},\bar X^{j+2}, \bar X^{j+3})) sgn(a(\bar Y^{j},\bar Y^{j+1},\bar Y^{j+2},\bar Y^{j+3}))
% \]
% By \ref{th:tau_star} $E \bar \tau_j=0$ if and only if  $\bar X_i , \bar Y_i$ are independent. Therefore, under the null hypothesis it is sufficient to show  that $ E (\bar \tau_j - \tau_j) = O(1/ \sqrt n)$. Indeed, 
% \[
%  \frac{1} {\sqrt n} \sum_{i}^{n} E (\tau_i - \bar \tau_j +\bar \tau_j) = \frac{1} {\sqrt n} \sum_{i}^{n} O(1/ \sqrt n) +0 \to 0.
% \]
% By the observation \ref{lemma:keyLemma} we see it is sufficient to prove that  
% \begin{align}
%    E sgn(a(X^{i},X^{i+1},X^{i+2},X^{i+3})) - sgn(a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})) \to a_i  \\
%   \sqrt n  E sgn(a(Y^{i},Y^{i+1},Y^{i+2},Y^{i+3})) - sgn(a(\bar Y^{i},\bar Y^{i+1},\bar Y^{i+2}, \bar Y^{i+3})) \to a_i
% \end{align}
% Without loss of generality we can consider only $X$'s. Let $A_n = a(X^{i},X^{i+1},X^{i+2},X^{i+3})$, $B_n = a(\bar X^{i},\bar X^{i+1},\bar X^{i+2}, \bar X^{i+3})$ . By the lemma \ref{lem:err} $ A_n-B_n \leq E_n = 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}|$. By the lemma  \ref{lem:bnd} it is enough to show that  $P( E_n > \delta_n ) = O(1/ \sqrt n)$ and $P( |B_n| < \delta_n )=O(1/ \sqrt n)$ for some  $\delta_n$. We choose $\delta_n = \frac{C} {n}$. For $B_n$ we have 
% \[
%  P( |B_n| < \frac{1}{\sqrt n} ) \leq \frac{C}{\sqrt n}. 
% \]
% By p-good assumption
% \begin{align}
%  P(|E_i| > \frac{1}{\sqrt n} ) = P( 2 \sum_{j=0}^3 |X_{i+j}-\bar X_{j+i}| > \frac{1}{\sqrt n} ) \leq \\
%  P( 8|Z^i - Z^{i+4}|^p > \frac{1}{\sqrt n} )   =  P(  8|Z^i - Z^{i+4}| > n^{-1/2p} ).
% \end{align}
% By (reference)  $D_{n,i} = |Z^i - Z^{i+4}|$ is distributed as $Beta(4,n-3)$.  One can show that for $p>0.5$ prove
% \[
%  \sqrt n P( n D_{n,i} > n^{-1/2p+1}) \to 0 
% \]
% Simple way to see it is true is to  notice that $n D(n,i)$ coverages to gamma(k,1) and we need $-1/2p+1 >0 $ so $n^{-1/2p+1}$ grows (any polynomial growth is OK since gamma has heavy tail).  $-1/2p+1 >0$ is equivalent to $p>0.5$. This is not proof since we used limit twice.
% \end{proof}
% 
% How to construct $\bar X^{i+j}$ ? We set $\bar X^{i+j} =  F^i(F^{i+j}(X^{i+j})$. Let 
% \[
%  P( |\bar X^{i+j} -X^{i+j}  |>a) = P( |F^i(F^{i+j}(X^{i+j}) -X^{i+j}  |>a) 
% \]
% clearly we need to make some assumptions  of $F$, for example of the following should do:  if $|z-z'| \leq a $
% \[
%  |x-x'| 
% \]







\bibliographystyle{plain}
\bibliography{acc}


\end{document}


